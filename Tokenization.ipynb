{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e529f88b",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n",
    "\n",
    "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f58c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "Hey Siri\n",
    "Hey Siri,! could you teach me how to cook an apple pie?\n",
    "Say thanks in Japanese??\n",
    "Make a paper plane that flies\n",
    "Hey Siri, could you cancel my alarm at eight o five?\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d560aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph ---> sentence\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706dae05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nHey Siri\\nHey Siri,!',\n",
       " 'could you teach me how to cook an apple pie?',\n",
       " 'Say thanks in Japanese??',\n",
       " 'Make a paper plane that flies\\nHey Siri, could you cancel my alarm at eight o five?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122c4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAragraph-->words\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b8c4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " 'Siri',\n",
       " 'Hey',\n",
       " 'Siri',\n",
       " ',',\n",
       " '!',\n",
       " 'could',\n",
       " 'you',\n",
       " 'teach',\n",
       " 'me',\n",
       " 'how',\n",
       " 'to',\n",
       " 'cook',\n",
       " 'an',\n",
       " 'apple',\n",
       " 'pie',\n",
       " '?',\n",
       " 'Say',\n",
       " 'thanks',\n",
       " 'in',\n",
       " 'Japanese',\n",
       " '?',\n",
       " '?',\n",
       " 'Make',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'plane',\n",
       " 'that',\n",
       " 'flies',\n",
       " 'Hey',\n",
       " 'Siri',\n",
       " ',',\n",
       " 'could',\n",
       " 'you',\n",
       " 'cancel',\n",
       " 'my',\n",
       " 'alarm',\n",
       " 'at',\n",
       " 'eight',\n",
       " 'o',\n",
       " 'five',\n",
       " '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71f39f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " 'Siri',\n",
       " 'Hey',\n",
       " 'Siri',\n",
       " ',!',\n",
       " 'could',\n",
       " 'you',\n",
       " 'teach',\n",
       " 'me',\n",
       " 'how',\n",
       " 'to',\n",
       " 'cook',\n",
       " 'an',\n",
       " 'apple',\n",
       " 'pie',\n",
       " '?',\n",
       " 'Say',\n",
       " 'thanks',\n",
       " 'in',\n",
       " 'Japanese',\n",
       " '??',\n",
       " 'Make',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'plane',\n",
       " 'that',\n",
       " 'flies',\n",
       " 'Hey',\n",
       " 'Siri',\n",
       " ',',\n",
       " 'could',\n",
       " 'you',\n",
       " 'cancel',\n",
       " 'my',\n",
       " 'alarm',\n",
       " 'at',\n",
       " 'eight',\n",
       " 'o',\n",
       " 'five',\n",
       " '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03de57dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " 'Siri',\n",
       " 'Hey',\n",
       " 'Siri',\n",
       " ',',\n",
       " '!',\n",
       " 'could',\n",
       " 'you',\n",
       " 'teach',\n",
       " 'me',\n",
       " 'how',\n",
       " 'to',\n",
       " 'cook',\n",
       " 'an',\n",
       " 'apple',\n",
       " 'pie',\n",
       " '?',\n",
       " 'Say',\n",
       " 'thanks',\n",
       " 'in',\n",
       " 'Japanese',\n",
       " '?',\n",
       " '?',\n",
       " 'Make',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'plane',\n",
       " 'that',\n",
       " 'flies',\n",
       " 'Hey',\n",
       " 'Siri',\n",
       " ',',\n",
       " 'could',\n",
       " 'you',\n",
       " 'cancel',\n",
       " 'my',\n",
       " 'alarm',\n",
       " 'at',\n",
       " 'eight',\n",
       " 'o',\n",
       " 'five',\n",
       " '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01d639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
